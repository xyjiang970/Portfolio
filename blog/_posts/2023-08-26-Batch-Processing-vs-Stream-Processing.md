---
layout: post
title: "Batch Processing vs Stream Processing"
author:
- Jason Jiang
permalink: 
---

[Home](../../../../) >> [Blog](../../../) >> Batch Processing vs Stream Processing

<h2 id="Contents">Contents</h2>
- [Batch Processing](#Batch Processing)
    - [Example Batch Processing Job Automations](#Example Batch Processing Job Automations)
    - [How Batch Processing Works](#How Batch Processing Works)
- [Stream Processing/ Streaming Data](#Stream Processing)
    - [Examples of Streaming Data](#Examples of Streaming Data)
    - [Benefits of Streaming Data](#Benefits of Streaming Data)
    - [Streaming Data Challenges](#Streaming Data Challenges)
- [Comparison Between Batch Processing and Stream Processing](#Comparison Between Batch Processing and Stream Processing)


<h2 id="#Batch Processing"><u><b>Batch Processing</b></u></h2>
Bath processingis the method computers use to periodically complete high-volume, repetitive data jobs. Certain data processing tasks (such as backups, filtering, and sorting) can be compute intensive and inefficient to run on individual data transactions. 

Instead, data systems process such tasks in batches, often in off-peak times when computing resources are more commonly available, such as at the end of the day or overnight.

For example, consider an ecommerce system that receives orders throughout the day. Instead of processing every order as it occurs, the system might collect all orders at the end of each day and share them in one batch with the order fulfillment team.

Organizations use batch processing because it requires minimal human interaction and makes repetitive tasks more efficient to run. You can set up batches of jobs composed of millions of records to be worked through together when compute power is most readily available, putting less stress on your systems. 
> With batch processing, some type of storage is required to load the data, such as a database or a file system.

<h3 id="Example Batch Processing Job Automations">Example Batch Processing Job Automations:</h3>
Common types of batch processing jobs include:
- Weekly/ monthly billing
- Payroll
- Report generation
- Data conversion
- Subscription cycles

<h3 id="How Batch Processing Works">How Batch Processing Works:</h3>
While batch processing applications vary depending on the type of task that needs to be done, the basics of any batch job remain the same. The user can run batch jobs by specifying the following details:
- Name of the person submitting the job
- Batch processes or programs that need to run
- System location of the data input
- System location for processed data output
- Time, or batch window, when the batch job should be run

The user also specifies the batch size, or the number of work units that the system needs to process in one complete batch operation. Some examples of batch size include: 
- Number of batch file lines to read and store in the database
- Number of messages to read and process from a queue
- Number of transactions to sort and send to the next application

**Dependencies**

Batch job tasks can run sequentially or simultaneously. Sequences can differ depending on whether an earlier task is completed successfully. Examples of dependencies include a customer making an order in an online store or paying a bill. A dependency can also be set up to initiate a job processing cycle.

**Cron commands**

A cron command is a batch job that runs regularly. You can set up recurrence patterns for batch jobs—for example, setting up a job to invoice for subscriptions at the end of every month.

![Batch Processing](https://www.upsolver.com/wp-content/uploads/2019/09/Slide1.png)

<br>
<hr>

<h2 id="#Stream Processing"><u><b>Stream Processing/ Streaming Data</b></u></h2>
Streaming data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). 

Streaming data includes a wide variety of data such as log files generated by customers using your mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services.

This data needs to be processed sequentially and incrementally on a record-by-record basis or over sliding time windows, and used for a wide variety of analytics including correlations, aggregations, filtering, and sampling. 

Information derived from such analysis gives companies visibility into many aspects of their business and customer activity such as –service usage (for metering/billing), server activity, website clicks, and geo-location of devices, people, and physical goods –and enables them to respond promptly to emerging situations.

For example, businesses can track changes in public sentiment on their brands and products by continuously analyzing social media streams, and respond in a timely fashion as the necessity arises.
> Stream processing is ideal for projects that require speed and nimbleness.

<h3 id="Examples of Streaming Data">Examples of Streaming Data:</h3>
- Sensors in transportation vehicles, industrial equipment, and farm machinery send data to a streaming application. The application monitors performance, detects any potential defects in advance, and places a spare part order automatically preventing equipment down time.
- A financial institution tracks changes in the stock market in real time, computes value-at-risk, and automatically rebalances portfolios based on stock price movements.
- A solar power company has to maintain power throughput for its customers, or pay penalties. It implemented a streaming data application that monitors of all of panels in the field, and schedules service in real time, thereby minimizing the periods of low throughput from each panel and the associated penalty payouts.
- A media publisher streams billions of clickstream records from its online properties, aggregates and enriches the data with demographic information about users, and optimizes content placement on its site, delivering relevancy and better experience to its audience.

<h3 id="Benefits of Streaming Data">Benefits of Streaming Data:</h3>
Streaming data processing is beneficial in most scenarios where new, dynamic data is generated on a continual basis. It applies to most of the industry segments and big data use cases.

Companies generally begin with simple applications such as collecting system logs and rudimentary processing like rolling min-max computations. Then, these applications evolve to more sophisticated near-real-time processing.

Initially, applications may process data streams to produce simple reports, and perform simple actions in response, such as emitting alarms when key measures exceed certain thresholds. Eventually, those applications perform more sophisticated forms of data analysis, like applying machine learning algorithms, and extract deeper insights from the data. 

Over time, complex, stream and event processing algorithms, like decaying time windows to find the most recent popular movies, are applied, further enriching the insights.

<h3 id="Streaming Data Challenges">Streaming Data Challenges:</h3>
Streaming data processing requires two layers: a storage layer and a processing layer.
- The storage layer needs to support record ordering and strong consistency to enable fast, inexpensive, and replayable reads and writes of large streams of data.
- The processing layer is responsible for consuming data from the storage layer, running computations on that data, and then notifying the storage layer to delete data that is no longer needed.

You also have to plan for scalability, data durability, and fault tolerance in both the storage and processing layers. As a result, many platforms have emerged that provide the infrastructure needed to build streaming data applications including: 
- Amazon Kinesis Data Streams
- Amazon Kinesis Data Firehose
- Amazon Managed Streaming for Apache Kafka (Amazon MSK)
- Amazon Managed Service for Apache Flink
- Apache Flume
- Apache Spark Streaming
- Apache Storm

<a href="https://aws.amazon.com/streaming-data/#Working_with_streaming_data_on_AWS" target="_blank">Working with streaming data on AWS.</a>

<br>
<hr>

<h2 id="Comparison Between Batch Processing and Stream Processing"><u><b>Comparison Between Batch Processing and Stream Processing</b></u></h2>
Whereas batch systems process large volumes of data and requests in **sequential order**, stream processing **continually** analyzes data that flows through a system or between devices.

Batch processing can be used to compute arbitrary queries over different sets of data. It usually computes results that are derived from all the data it encompasses, and enables deep analysis of big data sets.
> MapReduce-based systems, like Amazon EMR, are examples of platforms that support batch jobs.

In contrast, stream processing requires ingesting a sequence of data, and incrementally updating metrics, reports, and summary statistics in response to each arriving data record. The process monitors real-time data and continually passes it on in the network. It requires more processing power to monitor the large amounts of data.
> When the size of data being streamed is not known or infinite, streaming data can be preferable to batch processing. As a result, stream processing is commonly used for business functions such as cybersecurity, Internet of Things (IoT), personalized marketing services, and log monitoring.

<table>
  <tr>
    <th></th>
    <th>Batch processing</th>
    <th>Stream processing</th>
  </tr>
  <tr>
    <td>Data scope</td>
    <td>Queries or processing over all or most of the data in the dataset.</td>
    <td>Queries or processing over data within a rolling time window, or on just the most recent data record.</td>
  </tr>
  <tr>
    <td>Data size</td>
    <td>Large batches of data.</td>
    <td>Individual records or micro batches consisting of a few records.</td>
  </tr>
  <tr>
    <td>Performance</td>
    <td>Latencies in minutes to hours.</td>
    <td>Requires latency in the order of seconds or milliseconds.</td>
  </tr>
  <tr>
    <td>Analysis</td>
    <td>Complex analytics.</td>
    <td>Simple response functions, aggregates, and rolling metrics.</td>
  </tr>
</table>


<br>

<br>

[Back to Top](#)